{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baruch_GenderHeight_SportTeams.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHv5JhHWjmtw"
      },
      "source": [
        "# Goal: If varsity (college) swimmers are (on average) taller than their volleyball counterparts\n",
        "# Data: Baruch's, Brooklyn's, York's, and John Jay's volleyball and swimming teams\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htj13eZRgAg2"
      },
      "source": [
        "def webscrape (url):\n",
        "  #Store content retrieved from the url into page, and use beautifulsoup package to parse HTML\n",
        "  page = requests.get(url, verify=True)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  #Find span tag text with class of sidearm roster player height\n",
        "  heightlist= soup.find_all('span', class_= \"sidearm-roster-player-height\") \n",
        "  \n",
        "  list=[]\n",
        "  #The loop will run and find height of all players and append them into a list\n",
        "  for height in heightlist:\n",
        "    list.append(height.get_text())\n",
        "    #Use list comprehension to split the list and retreive the first element in the list, which is the height in feet\n",
        "    ft_list= [(num.split(\"'\")[0]) for num in list]\n",
        "    #Use list comprehension to split the list and retreive the second element in the list, which is the height in inches\n",
        "    inch_list= [(num.split(\"'\")[1]) for num in list]\n",
        "    #Then, use list comprehension so there is just numbers in the list, no special charc.\n",
        "    inch_list= [char.replace('\"', '') for char in inch_list]\n",
        "    #Use loop: for each number in the list, convert them to an int and multiply them to convert to cm \n",
        "    for num in range(0, len(ft_list)):\n",
        "      ft_list[num]= int(ft_list[num])*30.48\n",
        "      inch_list[num]= int(inch_list[num])*2.54\n",
        "  \n",
        "  #Find average of the two lists and add them together to find average height of all players\n",
        "  height1=(sum(ft_list)/len(ft_list))\n",
        "  height2=(sum(inch_list)/len(inch_list))\n",
        "  print(\"The average height of all players is:\", height1+height2, \"cm\")\n",
        "\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZKgFMPq6yRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afcc4f70-c077-458d-ada0-4e910bdce7da"
      },
      "source": [
        "#Define websites in list since there's more than one website we scrape from \n",
        "websites=[\"https://athletics.baruch.cuny.edu/sports/mens-swimming-and-diving/roster\", \"https://athletics.baruch.cuny.edu/sports/mens-volleyball/roster\", \"https://athletics.baruch.cuny.edu/sports/womens-swimming-and-diving/roster\", \"https://athletics.baruch.cuny.edu/sports/womens-volleyball/roster\"]\n",
        "\n",
        "#Use list comprehension to run each url in the websites list through the function above\n",
        "heights=[webscrape(url) for url in websites]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average height of all players is: 180.22956521739124 cm\n",
            "The average height of all players is: 186.76470588235298 cm\n",
            "The average height of all players is: 163.90470588235289 cm\n",
            "The average height of all players is: 165.94666666666672 cm\n"
          ]
        }
      ]
    }
  ]
}